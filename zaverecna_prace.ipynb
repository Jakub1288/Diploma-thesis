{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26216311-2c28-4f98-9947-07612fe665e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os # to get ahold of the basic operating system functionalities\n",
    "from glob import glob # to get the filepaths\n",
    "from constance import table_of_contents #Table of content is just dictionary with possible headings for Table of contents for Czech Lang.\n",
    "import re # to use regular expressions\n",
    "import pdfplumber #To import text from PDF to pandas Dataframe\n",
    "import numpy as np #To do basic math operation\n",
    "import matplotlib.pyplot as plt # to plot the graphs\n",
    "import pickle # to open and load with pickle\n",
    "import string #To remove punctation\n",
    "import collections # To count the occurrences of each word\n",
    "\n",
    "pd.options.display.max_colwidth=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df8fb5-0f72-478b-840a-8d95e95beadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_years=[2019, 2020, 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7fd30-22a2-4142-b95a-780c49191fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfcrs = dict() # Sfcr dict with key reporting_year stores filepaths\n",
    "for reporting_year in reporting_years:\n",
    "    sfcrs[reporting_year] = glob(('\\\\').join([os.path.normpath('MyPath'), 'reports', str(reporting_year), '*.pdf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6d2a1-9d5c-4ac8-82ff-db10ed7b08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'MyPath' # creating text_splits folder\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.path.normpath(os.path.dirname(file) + '\\\\text_splits'))\n",
    "except OSError:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd8e03-9949-4a2f-b573-1efc897bb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the text to be stored in dataframes\n",
    "extracted_text = dict()\n",
    "number_of_pages=[]\n",
    "try:\n",
    "    with open ((os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\number_of_pages.pkl')), 'rb') as f:\n",
    "            number_of_pages = pickle.load(f)    \n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "for reporting_year in reporting_years:\n",
    "    try:\n",
    "        extracted_text[reporting_year] = pd.read_pickle(os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\extracted_text_(' + str(reporting_year) + ').pkl')).fillna(value=np.nan)\n",
    "    except FileNotFoundError:\n",
    "        extracted_text[reporting_year] = pd.DataFrame()\n",
    "        for filepath in sfcrs[reporting_year]:\n",
    "            toc_page = 0\n",
    "            undertaking_code = int(re.findall('\\d+', filepath)[-1])\n",
    "            with pdfplumber.open(filepath) as pdf_file:\n",
    "                try:\n",
    "                    number_of_pages.append(len(pdf_file.pages))\n",
    "                    for page in pdf_file.pages:\n",
    "                        if re.search(table_of_contents['cz'].replace(' ', '.*'), page.extract_text(), re.IGNORECASE):\n",
    "                            toc_page = page.page_number - 1 #If the text is found- in table_of_contents['cz'], the page number is stored in the toc_page variable and the loop is broken.\n",
    "                            break\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                actual_page = toc_page\n",
    "                if toc_page:\n",
    "                    toc_numbers = re.findall('\\d+', pdf_file.pages[toc_page].extract_text(), flags=re.IGNORECASE) #This part of the code is checking the page numbers listed in the table of contents, and using the last page number listed to determine the starting page for processing the text in the PDF file.\n",
    "                    if toc_numbers and (int(toc_numbers[-1])-1 <= toc_page):\n",
    "                        actual_page = int(toc_numbers[-1])-1\n",
    "                try:\n",
    "                    for page in pdf_file.pages[actual_page:]:\n",
    "                        extracted_text[reporting_year].at[actual_page, undertaking_code] = page.extract_text()\n",
    "                        actual_page += 1\n",
    "                except TypeError:\n",
    "                    pass\n",
    "        for undertaking_code in extracted_text[reporting_year]:\n",
    "            if ((extracted_text[reporting_year][undertaking_code].values == '').sum() + extracted_text[reporting_year][undertaking_code].isna().sum()) == len(extracted_text[reporting_year].index):\n",
    "                extracted_text[reporting_year].drop(undertaking_code, axis=1, inplace=True)\n",
    "        extracted_text[reporting_year].to_pickle(os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\extracted_text_(' + str(reporting_year) + ').pkl'))\n",
    "\n",
    "with open((os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\number_of_pages.pkl')),'wb')  as f:\n",
    "    pickle.dump(number_of_pages, f)    #If we run this cell pickle will overwrite this again by same pickle    \n",
    "print(number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d4da9-0011-4054-89c8-dd27c501b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_maximum=np.max(number_of_pages)\n",
    "pages_minimum=np.min(number_of_pages)\n",
    "average_pages=np.mean(number_of_pages)\n",
    "print(pages_maximum, pages_minimum, average_pages)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars=plt.bar(['maximum', 'průměr', 'minimum'], [pages_maximum, average_pages, pages_minimum], width=0.4)\n",
    "plt.ylabel('Počet stran')\n",
    "\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height/2, '%d'  %int(height),\n",
    "             ha='center', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09400147-645d-4155-91aa-95043891521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making dictionary with quant features and target feature\n",
    "quantitative_feature_str = {}\n",
    "\n",
    "for reporting_year in reporting_years:\n",
    "    file_path = \"Quantitative_features.xlsx\" # Construct the file path\n",
    "    \n",
    "    quantitative_feature_str[reporting_year] = pd.read_excel(file_path, sheet_name=str(reporting_year)) # Load the sheet with the given reporting year into a dataframe\n",
    "    quantitative_feature_str[reporting_year]=quantitative_feature_str[reporting_year].drop(columns='Solvency_ratio_from_next_year')\n",
    "    quantitative_feature_str[reporting_year].set_index('Pojistovna_ID', inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e6dda-ffa9-4b4e-adad-f35a19a974a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the features thata have more then 40% of NaN values\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "quantitative_feature_all_years = pd.DataFrame(columns=quantitative_feature_str[reporting_years[0]].columns) \n",
    "\n",
    "for reporting_year in reporting_years:\n",
    "    df_temp = quantitative_feature_str[reporting_year] #temporary dataframe for each reporting year\n",
    "    quantitative_feature_all_years = quantitative_feature_all_years.append(df_temp) #merge all the years together\n",
    "\n",
    "percentage_nan_values_all_years = quantitative_feature_all_years.isna().mean(axis=0) * 100\n",
    "\n",
    "columns_to_drop = percentage_nan_values_all_years[percentage_nan_values_all_years > 40].index.tolist()\n",
    "for reporting_year in reporting_years:\n",
    "    quantitative_feature_str[reporting_year] = quantitative_feature_str[reporting_year].drop(columns=columns_to_drop) #dropping columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f159d-1230-40ef-ac0b-68e58df24ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the list of quantitative columns to be use later in mixed model\n",
    "columns_of_quant_features = list(quantitative_feature_str[next(iter(quantitative_feature_str))].columns) \n",
    "print(columns_of_quant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc802b0-d1ab-433a-b572-c02741602c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating full_text dataframe that merge all the reporting years together and also joining text and quantitative features\n",
    "full_only_text = pd.DataFrame(columns=['Undertaking Code', 'Text'])\n",
    "\n",
    "for year, values in extracted_text.items():\n",
    "    if year in quantitative_feature_str.keys():  # check if there are quantitative features for this year\n",
    "        df_qf = quantitative_feature_str[year]  # get the quantitative feature dataframe for this year\n",
    "        for undertaking_code, texts in values.items():\n",
    "            texts = [str(text) for text in texts]\n",
    "            if undertaking_code in df_qf.index:  # check if there are quantitative features for this undertaking code\n",
    "                row = {'Undertaking Code': str(undertaking_code) + '_' + str(year), 'Text': \" \".join(texts)}\n",
    "                row.update(df_qf.loc[undertaking_code].to_dict())  # add quantitative features to the row\n",
    "                full_only_text = full_only_text.append(row, ignore_index=True)\n",
    "\n",
    "full_text = full_only_text.set_index('Undertaking Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9361a4d-0e0a-486a-91a8-3c3ca5e168ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[columns_of_quant_features] = full_text[columns_of_quant_features].fillna(full_text[columns_of_quant_features].mean()) #Filling missing values in indicator columns by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66b9f2-24ff-42d5-8f66-6c6b6d18bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening text file containing stopwords\n",
    "with open('stopwords.txt', 'r', encoding='UTF-8') as file_stopwords:\n",
    "    text = file_stopwords.read()\n",
    "\n",
    "stop_words = text.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716abc67-c289-4a97-b285-fb989c9f5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#Remove Punctation\n",
    "def remove_punctuation(txt):\n",
    "    text_nopunct =\"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "#Tokenize text using nltk library\n",
    "def tokenize(txt):\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    return tokens\n",
    "\n",
    "#Remove tokens that include numbers\n",
    "def to_lower(tokens):\n",
    "    lower_letters = [token.lower() for token in tokens]\n",
    "    return lower_letters\n",
    "\n",
    "#Remove words with less than 3 letters and including numbers\n",
    "def remove_short(tokens):\n",
    "    text_no_stop = [token for token in tokens if (len(token) > 2) and (not any(char.isdigit() for char in token))]\n",
    "    return text_no_stop\n",
    "\n",
    "#Remove stopwords stored in txt file\n",
    "def remove_stopwords(tokens, stop_words):\n",
    "    text_no_stop=[token for token in tokens if token not in stop_words]\n",
    "    return text_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a7be9-b158-4d17-910c-29c62a01c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['Text_nopunct']=full_text['Text'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eea695-eda7-4ee3-8707-1b5178da0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['Text_tokenized']=full_text['Text_nopunct'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cb363-24a8-4413-bd1e-c5189e28810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['lower_only']=full_text['Text_tokenized'].apply(lambda x: to_lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d200475-81f8-4034-a88f-e33ee022c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['No_single_letter']=full_text['lower_only'].apply(lambda x: remove_short(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d5b8e-47f4-4a3e-a0e7-cb65eb2e02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['No_stop_words']=full_text['No_single_letter'].apply(lambda x: remove_stopwords(x, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f8b77-5f19-430d-9bc2-0a9d27e4f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text\n",
    "#full_text.drop(['Text_nopunct', 'Text','Text_tokenized','lower_only', 'No_single_letter', 'No_stop_words'] , axis=1).rename(columns={\"five_letters\": \"cleaned_text\"}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019452b-4cf7-4857-aba8-474452aeb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd8fab-532a-4e90-a886-40f1adca3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_five_letters(token_list):\n",
    "    first_five = [token[:5] for token in token_list] \n",
    "    return first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218a843-15c3-4dc9-b71b-c9a7d90ea9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[\"five_letters\"] = full_text[\"No_stop_words\"].apply(lambda x: first_five_letters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b04d8b-bf7c-44b4-8316-c8abc6a17f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = full_text[\"five_letters\"].apply(lambda x: \" \".join(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5234f-3bec-4a25-ac96-48e8d2d22572",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_no_stop = full_text['No_stop_words'].apply(lambda x: \" \".join(x)).tolist()\n",
    "text_no_stop = ' '.join(text_list_no_stop)\n",
    "\n",
    "words = text_no_stop.split() # Split the string into words\n",
    "\n",
    "unique_words = set(words) # Get unique words\n",
    "\n",
    "print(\"počet unikátních slov je: \",(len(unique_words)))\n",
    "print(\"počet slov celkem po odstranění stop slov je: \",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394e5b5-950e-4ea9-b3e2-9cda17a25077",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_lower_only = full_text['lower_only'].apply(lambda x: \" \".join(x)).tolist()\n",
    "text_no_stop = ' '.join(text_list_lower_only)\n",
    "\n",
    "words_before_cleaning = text_no_stop.split() # Split the string into words\n",
    "\n",
    "unique_words_before_cleaning = set(words_before_cleaning) # Get unique words\n",
    "\n",
    "print(\"počet unikátních slov před čištěním textu je: \",(len(unique_words_before_cleaning)))\n",
    "print(\"počet slov před čištěním textu celkem je: \",len(words_before_cleaning))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc0de6-a898-4e0b-bdf5-7b0917a80864",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_stop_five_lett = ' '.join(text_list)\n",
    "words = text_no_stop_five_lett.split() \n",
    "unique_words = set(words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee604a-d48b-4d0d-b82a-8fd5038f514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text_no_stop_five_lett.split() # Split the string into words\n",
    "word_counts = collections.Counter(words) # Count the occurrences of each word\n",
    "one_word_occurrence = [word for word, count in word_counts.items() if count == 1]\n",
    "print(len(one_word_occurrence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade69f7-0dd9-4eb0-ae60-a65f80fe6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(text.split()) for text in text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a513b6-d65d-4849-9137-a360b2621f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading or crating text list ready excluded one word occurence words\n",
    "try:\n",
    "    with open ((os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\text_list_ready.pkl')), 'rb') as f:\n",
    "            text_list_ready = pickle.load(f)  \n",
    "except:\n",
    "    text_list_ready=[]\n",
    "    for text in text_list:\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in one_word_occurrence]\n",
    "        filtered_text = ' '.join(filtered_words)\n",
    "        text_list_ready.append(filtered_text)\n",
    "            \n",
    "    with open((os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\text_list_ready.pkl')),'wb')  as f:\n",
    "        pickle.dump(text_list_ready, f)     \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4a262-b9a3-4cfb-9c2c-823ef18606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(len(text.split()) for text in text_list_ready)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b1ffb-cddc-4661-8c90-638e5b93ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdd4d0-ac3e-462c-8061-363dee47fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_after_cleaning_no_list=' '.join(text_list_ready)\n",
    "\n",
    "words_after_cleaning = words_after_cleaning_no_list.split() # Split the string into words\n",
    "\n",
    "len(set(words_after_cleaning)) # Get unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcc246-f217-4a41-86c6-d3d8eac099c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf.fit(text_list_ready) # Fit the vectorizer to the text data\n",
    "tfidf_matrix = tfidf.transform(text_list_ready) # Transform the text data into a TF-IDF matrix\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names(), index=full_text.index) # Convert the matrix to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5800ab-b084-4cac-8ed9-eddfb802e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21d3c3-f9f5-48db-a6c3-dd2c39772f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_final = tfidf_df.join(full_text[columns_of_quant_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61126e-544e-4515-b18a-d5ce6ab1afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a2f60-df5f-469b-8e40-c612151f1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df_final[tfidf_df_final.columns.difference(['Solvency_ratio_bucket'])]\n",
    "y = tfidf_df_final['Solvency_ratio_bucket']\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1910b6e-c40f-4127-bac8-04237374b05e",
   "metadata": {},
   "source": [
    "### Training SVM for selected Ngram, All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46149-6e67-45cb-a4ff-06cf22b1c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear')  \n",
    "svm_model.fit(X, y)\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "accuracy_scores = cross_val_score(svm_model, X, y, scoring = accuracy, cv = 7)\n",
    "\n",
    "\n",
    "print(\"accuracy score:\", accuracy_scores)\n",
    "print(\"accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c13199-a3b4-413c-ac5e-3c6790e03f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a08ec-2144-428f-b7f0-33ae9db8decc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training SVM for selected Ngram, X % most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a68508-384d-4e09-b7b6-3ab77179325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_matrix.shape)\n",
    "print(svm_model.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfb333-daa9-48b9-8cc6-af68c52d7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only best 80% of features\n",
    "features_coef = dict(zip(list(tfidf.get_feature_names()) + list(full_text[columns_of_quant_features].columns), list(svm_model.coef_[0]) + [0]*len(columns_of_quant_features))) # get feature names and coefficients, features must be from tfidf and also connected with quantitative variables\n",
    "top_features = sorted(features_coef.items(), key=lambda x: abs(x[1]), reverse=True)[:int(len(features_coef)*0.9)] # sort by absolute coefficient value and select top X %\n",
    "top_feature_names = [x[0] for x in top_features] # extract top feature names\n",
    "\n",
    "\n",
    "X_train_top_x = X[top_feature_names] # subset the data to include only top X % features\n",
    "\n",
    "svm_model_top_x = SVC(kernel='linear')  # train the model on the subset of top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39d09b-942e-48ed-9064-f3aca9321e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(svm_model_top_x, X_train_top_x, y, scoring = accuracy, cv = 7)\n",
    "\n",
    "print(\"accuracy score:\", accuracy_scores)\n",
    "print(\"accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21b2de-1cb1-4e64-9a1f-bd87d4eae083",
   "metadata": {},
   "source": [
    "## KNN, selected N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8770a2-dfe0-4917-8c06-50d16d5c0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = tfidf_df_final[tfidf_df_final.columns.difference(['Solvency_ratio_bucket'])]\n",
    "y = tfidf_df_final['Solvency_ratio_bucket']\n",
    "\n",
    "print(y.value_counts())\n",
    "\n",
    "knn= KNeighborsClassifier()\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(knn, X, y, scoring = accuracy, cv = 7)\n",
    "\n",
    "\n",
    "print(\"accuracy score:\", accuracy_scores)\n",
    "print(\"accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35e023-3713-4700-8504-044eb887e1df",
   "metadata": {},
   "source": [
    "## selected N-gram, X % most importanat features using ANNOVA f-value, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec81b9e-6387-45bf-815b-99f3ac08ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "percentile = 20 # Select the top X % of features based on ANOVA F-value\n",
    "selector = SelectPercentile(f_classif, percentile=percentile)\n",
    "selector.fit(tfidf_matrix, y)\n",
    "tfidf_selected = selector.transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a8469-0bad-4aa5-a70c-34560f42afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = selector.get_support(indices=True) # Get the indices of the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af52ad4-3333-4878-969a-b691fa56335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_selected = tfidf_df.iloc[:, selected_indices]\n",
    "tfidf_df_final = tfidf_df_selected.join(full_text[columns_of_quant_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86818a-c7f1-43ae-a7cb-77e8b19f6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier()\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(knn, X, y, scoring = accuracy, cv = 7)\n",
    "\n",
    "print(\"precision score:\", accuracy_scores)\n",
    "print(\"accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a723d5f-b58b-44ca-9009-1f999440c075",
   "metadata": {},
   "source": [
    "## Logistic regression, selected N - grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69389750-b0eb-4151-9323-5c34832c8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression(C=10)\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(lr, X, y, scoring=accuracy, cv=7)\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120cc0c-d930-4e1c-95ff-c13c090d7ef2",
   "metadata": {},
   "source": [
    "### Logistic regression, selected N - grams, Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14edaf4-1127-46a1-a655-ebc47cf7c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)\n",
    "\n",
    "feature_names = tfidf_df_final.columns.difference(['Solvency_ratio_bucket']) # Get the feature names from the original DataFrame\n",
    "\n",
    "\n",
    "coef_abs = np.abs(lr.coef_[0]) # Compute the absolute value of the coefficients\n",
    "sorted_idx = np.argsort(coef_abs)[::-1] # Sort the coefficients in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbaa86-37a9-4b01-ac7c-ebfb7c04c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(feature_names)\n",
    "\n",
    "n_keep = int(n_features * 0.8) # Compute the number of features to keep\n",
    "\n",
    "top_idx = sorted_idx[:n_keep] # Get the indices of the top n_keep features\n",
    "\n",
    "X_lr_top_X = X.iloc[:, top_idx] # Create a new DataFrame with only the top features\n",
    "\n",
    "print(f\"Original number of features: {n_features}\")\n",
    "print(f\"Number of features to keep: {n_keep}\")\n",
    "print(f\"New number of features: {X_lr_top_X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907f58d-ffd1-493f-b27d-d3610c517c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(lr, X_lr_top_X, y, scoring=accuracy, cv=7)\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51682f82-bd25-4bc9-b504-707fd6270711",
   "metadata": {},
   "source": [
    "## Decision tree, Selected N - grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1b0f6-5131-4954-9123-c5ae50fbb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(dtc, X, y, scoring=accuracy, cv=7)\n",
    "\n",
    "# Print the mean and standard deviation of the accuracy scores\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474ce798-538d-4150-bec0-5d8c19dc6114",
   "metadata": {},
   "source": [
    "### Decision tree, Selected N - grams, Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0eac7-1868-4998-bb09-7a66a9a82aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "dtc.fit(X, y)\n",
    "importances = dtc.feature_importances_\n",
    "\n",
    "\n",
    "sorted_idx = np.argsort(importances)[::-1] # Get indices of the top features sorted in descending order of importance\n",
    "\n",
    "n_features = len(importances)\n",
    "n_keep = int(n_features * 0.2) # Compute the number of features to keep\n",
    "\n",
    "top_idx = sorted_idx[:n_keep] # Get the indices of the top n_keep features\n",
    "\n",
    "X_dtc_top_X = X.iloc[:, top_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57662680-e4f1-4528-aa73-fccf532d562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = cross_val_score(dtc, X_dtc_top_X, y, scoring=accuracy, cv=7)\n",
    "\n",
    "# Print the mean and standard deviation of the accuracy scores\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b8d98-0e88-4a40-a52e-6c680e00da47",
   "metadata": {},
   "source": [
    "## Naive Bayes, Selected N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a31d8-6336-4c6b-952f-117137cb25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb=GaussianNB()  \n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(nb, X, y, scoring=accuracy, cv=7)\n",
    "\n",
    "# Print the mean and standard deviation of the accuracy scores\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b046fd-5add-449f-af16-f94fbf6fe86c",
   "metadata": {},
   "source": [
    "## Neural network, Selected N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cfc53-0f02-425b-9be2-b352e403e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.replace({'positive': 1, 'stable': 0})\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683671ec-a325-4cc3-9f35-a2a3222ccec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "k = 7\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "\n",
    "accuracy_scores = []\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))\n",
    "\n",
    "    accuracy_scores.append(history.history['val_accuracy'][-1])\n",
    "\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Accuracy score:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b099ed3-255b-49c4-b071-0b5d9444178b",
   "metadata": {},
   "source": [
    "# SECOND PART: USING TEST SET, HYPERPARAMETER TUNING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747063c-5404-494a-ac17-e4471a9f3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfcrs = dict() # Sfcr dict with key reporting_year stores filepaths\n",
    "for reporting_year in reporting_years:\n",
    "    sfcrs[reporting_year] = glob(('\\\\').join([os.path.normpath('C:\\\\Users\\\\jakub\\\\Desktop\\\\PythonPrecinok\\\\NLTK\\\\env\\\\NLTK\\\\Scripts\\\\MLProject'), 'reports', str(reporting_year), '*.pdf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9387e50-e4c0-4125-9170-0e6bf0d56088",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'C:\\\\Users\\\\jakub\\\\Desktop\\\\PythonPrecinok\\\\NLTK\\\\env\\\\NLTK\\\\Scripts\\\\MLProject\\\\' # creating text_splits folder\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.path.normpath(os.path.dirname(file) + '\\\\text_splits'))\n",
    "except OSError:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90eb97e-3ff0-4c4b-8f61-62f7b9aee559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the text to be stored in dataframes\n",
    "extracted_text = dict()\n",
    "number_of_pages=[]\n",
    "try:\n",
    "    with open ((os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\number_of_pages.pkl')), 'rb') as f:\n",
    "            number_of_pages = pickle.load(f)    \n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "for reporting_year in reporting_years:\n",
    "    try:\n",
    "        extracted_text[reporting_year] = pd.read_pickle(os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\extracted_text_(' + str(reporting_year) + ').pkl')).fillna(value=np.nan)\n",
    "    except FileNotFoundError:\n",
    "        extracted_text[reporting_year] = pd.DataFrame()\n",
    "        for filepath in sfcrs[reporting_year]:\n",
    "            toc_page = 0\n",
    "            undertaking_code = int(re.findall('\\d+', filepath)[-1])\n",
    "            with pdfplumber.open(filepath) as pdf_file:\n",
    "                try:\n",
    "                    number_of_pages.append(len(pdf_file.pages))\n",
    "                    for page in pdf_file.pages:\n",
    "                        if re.search(table_of_contents['cz'].replace(' ', '.*'), page.extract_text(), re.IGNORECASE):\n",
    "                            toc_page = page.page_number - 1 #If the text is found- in table_of_contents['cz'], the page number is stored in the toc_page variable and the loop is broken.\n",
    "                            break\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                actual_page = toc_page\n",
    "                if toc_page:\n",
    "                    toc_numbers = re.findall('\\d+', pdf_file.pages[toc_page].extract_text(), flags=re.IGNORECASE) #This part of the code is checking the page numbers listed in the table of contents, and using the last page number listed to determine the starting page for processing the text in the PDF file.\n",
    "                    if toc_numbers and (int(toc_numbers[-1])-1 <= toc_page):\n",
    "                        actual_page = int(toc_numbers[-1])-1\n",
    "                try:\n",
    "                    for page in pdf_file.pages[actual_page:]:\n",
    "                        extracted_text[reporting_year].at[actual_page, undertaking_code] = page.extract_text()\n",
    "                        actual_page += 1\n",
    "                except TypeError:\n",
    "                    pass\n",
    "        for undertaking_code in extracted_text[reporting_year]:\n",
    "            if ((extracted_text[reporting_year][undertaking_code].values == '').sum() + extracted_text[reporting_year][undertaking_code].isna().sum()) == len(extracted_text[reporting_year].index):\n",
    "                extracted_text[reporting_year].drop(undertaking_code, axis=1, inplace=True)\n",
    "        extracted_text[reporting_year].to_pickle(os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\extracted_text_(' + str(reporting_year) + ').pkl'))\n",
    "\n",
    "with open((os.path.normpath(os.path.dirname(file) + '\\\\text_splits\\\\number_of_pages.pkl')),'wb')  as f:\n",
    "    pickle.dump(number_of_pages, f)    #If we run this cell pickle will overwrite this again by same pickle    \n",
    "print(number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2635b98-4a57-4c27-9537-bebe70953b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making dictionary with quant features and target feature\n",
    "quantitative_feature_str = {}\n",
    "\n",
    "for reporting_year in reporting_years:\n",
    "    file_path = \"Quantitative_features.xlsx\" # Construct the file path\n",
    "    \n",
    "    quantitative_feature_str[reporting_year] = pd.read_excel(file_path, sheet_name=str(reporting_year)) # Load the sheet with the given reporting year into a dataframe\n",
    "    quantitative_feature_str[reporting_year]=quantitative_feature_str[reporting_year].drop(columns='Solvency_ratio_from_next_year')\n",
    "    quantitative_feature_str[reporting_year].set_index('Pojistovna_ID', inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283d3f4-16f8-4812-9517-1e3fc26decf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the features thata have more then 40% of NaN values\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "quantitative_feature_all_years = pd.DataFrame(columns=quantitative_feature_str[reporting_years[0]].columns) \n",
    "\n",
    "for reporting_year in reporting_years:\n",
    "    df_temp = quantitative_feature_str[reporting_year] #temporary dataframe for each reporting year\n",
    "    quantitative_feature_all_years = quantitative_feature_all_years.append(df_temp) #merge all the years together\n",
    "\n",
    "percentage_nan_values_all_years = quantitative_feature_all_years.isna().mean(axis=0) * 100\n",
    "\n",
    "columns_to_drop = percentage_nan_values_all_years[percentage_nan_values_all_years > 40].index.tolist()\n",
    "for reporting_year in reporting_years:\n",
    "    quantitative_feature_str[reporting_year] = quantitative_feature_str[reporting_year].drop(columns=columns_to_drop) #dropping columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e80c8-7249-4913-9343-97d37c72406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the list of quantitative columns to be use later in mixed model\n",
    "columns_of_quant_features = list(quantitative_feature_str[next(iter(quantitative_feature_str))].columns) \n",
    "print(columns_of_quant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12942707-aa82-4e0d-aa10-5e7e220fc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating full_text dataframe that merge all the reporting years together and also joining text and quantitative features\n",
    "full_only_text = pd.DataFrame(columns=['Undertaking Code', 'Text'])\n",
    "\n",
    "for year, values in extracted_text.items():\n",
    "    if year in quantitative_feature_str.keys():  # check if there are quantitative features for this year\n",
    "        df_qf = quantitative_feature_str[year]  # get the quantitative feature dataframe for this year\n",
    "        for undertaking_code, texts in values.items():\n",
    "            texts = [str(text) for text in texts]\n",
    "            if undertaking_code in df_qf.index:  # check if there are quantitative features for this undertaking code\n",
    "                row = {'Undertaking Code': str(undertaking_code) + '_' + str(year), 'Text': \" \".join(texts)}\n",
    "                row.update(df_qf.loc[undertaking_code].to_dict())  # add quantitative features to the row\n",
    "                full_only_text = full_only_text.append(row, ignore_index=True)\n",
    "\n",
    "full_text = full_only_text.set_index('Undertaking Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafe434-687f-4a7f-8644-4190a6242014",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_train=full_text.iloc[:48,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48823a-e445-4c85-ab31-63021c386136",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[columns_of_quant_features] = full_text[columns_of_quant_features].fillna(full_text_train[columns_of_quant_features].mean()) #Filling missing values in indicator columns by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107298f5-5e28-40c1-8963-efd077a0467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r', encoding='UTF-8') as file_stopwords:\n",
    "    text = file_stopwords.read()\n",
    "\n",
    "stop_words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892e904-5a95-447c-9762-3a8687a09197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "#Remove Punctation\n",
    "def remove_punctuation(txt):\n",
    "    text_nopunct =\"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "#Tokenize text using nltk library\n",
    "def tokenize(txt):\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    return tokens\n",
    "\n",
    "#Remove tokens that include numbers\n",
    "def to_lower(tokens):\n",
    "    lower_letters = [token.lower() for token in tokens]\n",
    "    return lower_letters\n",
    "\n",
    "#Remove words with less than 3 letters and including numbers\n",
    "def remove_short(tokens):\n",
    "    text_no_stop = [token for token in tokens if (len(token) > 2) and (not any(char.isdigit() for char in token))]\n",
    "    return text_no_stop\n",
    "\n",
    "#Remove stopwords stored in txt file\n",
    "def remove_stopwords(tokens, stop_words):\n",
    "    text_no_stop=[token for token in tokens if token not in stop_words]\n",
    "    return text_no_stop\n",
    "\n",
    "#Shorten the long words to their first 5 letters\n",
    "def first_five_letters(token_list):\n",
    "    first_five = [token[:5] for token in token_list] \n",
    "    return first_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4a934-dfbd-459b-a971-0879082e50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['Text_nopunct']=full_text['Text'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392da626-63b0-4ef1-8ada-a8e13ce41fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['Text_tokenized']=full_text['Text_nopunct'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db85ae1-6b1c-452b-b52f-1b187827336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['lower_only']=full_text['Text_tokenized'].apply(lambda x: to_lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd328a6-ef44-4e97-8472-9081b04087c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['No_single_letter']=full_text['lower_only'].apply(lambda x: remove_short(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c745a-e79a-4533-ab81-737a87b6e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text['No_stop_words']=full_text['No_single_letter'].apply(lambda x: remove_stopwords(x, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324166a6-4f6e-4b3f-964a-1cc41e26d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text[\"five_letters\"] = full_text[\"No_stop_words\"].apply(lambda x: first_five_letters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa177b-a1ee-4979-b92c-57aead1f25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = full_text[\"five_letters\"].apply(lambda x: \" \".join(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b261f-0ca8-42a7-8b2f-f4df39dafb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no_stop_five_lett = ' '.join(text_list)\n",
    "\n",
    "words = text_no_stop_five_lett.split() # Split the string into words\n",
    "\n",
    "unique_words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f501a8-af99-4150-a1d2-814943d12f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text_no_stop_five_lett.split() # Split the string into words\n",
    "word_counts = collections.Counter(words) # Count the occurrences of each word\n",
    "\n",
    "one_word_occurrence = [word for word, count in word_counts.items() if count == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996db92-27ad-4e12-8b74-d2112a3c495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_ready=[]\n",
    "for text in text_list:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in one_word_occurrence]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    text_list_ready.append(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd240c1a-9253-4055-bac0-bc436a2d1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(3,3))\n",
    "tfidf.fit(text_list_ready) # Fit the vectorizer to the text data\n",
    "tfidf_matrix = tfidf.transform(text_list_ready) # Transform the text data into a TF-IDF matrix\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names(), index=full_text.index) # Convert the matrix to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375c9b6-72d9-48c2-86fe-af188ebc3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_final = tfidf_df.join(full_text[columns_of_quant_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742d9d3-dce0-4764-abed-ae7fc6a04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_final_test=tfidf_df_final.iloc[48:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3973516-aa4f-4ab5-9b9f-ed27eab2003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_final=tfidf_df_final.iloc[:48,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add98ec-e17f-41c9-8654-555a120a3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_text_list_ready = text_list_ready[:48] # Get the subset of text data for the first 48 rows\n",
    "subset_tfidf_matrix = tfidf.transform(subset_text_list_ready) # Transform the subset of text data into a TF-IDF matrix\n",
    "\n",
    "subset_tfidf_df = pd.DataFrame(subset_tfidf_matrix.toarray(), columns=tfidf.get_feature_names()) # Convert the matrix to a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318fc65-094a-484b-b594-a59c69605ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df_final[tfidf_df_final.columns.difference(['Solvency_ratio_bucket'])]\n",
    "y = tfidf_df_final['Solvency_ratio_bucket']\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d169f-3318-40e4-a941-f43e7f1dcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_df_final_test[tfidf_df_final_test.columns.difference(['Solvency_ratio_bucket'])]\n",
    "y_test = tfidf_df_final_test['Solvency_ratio_bucket']\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166adf5b-dbb0-4d22-8c73-2c1f773d00aa",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5abe63-08cc-4743-bce4-a5bba422b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "param_grid = {'penalty': [ 'none', 'l1', 'l2', 'elasticnet'], 'C': [ 0.1, 1, 2, 3, 4, 10], 'solver':[ 'lbfgs' ,'liblinear'] }\n",
    "\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=LeaveOneOut())\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model's performance with cross-validation\n",
    "accuracy_scores = cross_val_score(grid_search.best_estimator_, X, y, cv=LeaveOneOut())\n",
    "\n",
    "# Print the mean and standard deviation of the accuracy scores\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520addf-768c-4cc0-94f3-ac549bcca244",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on testing data:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586862c-94a3-4635-9a86-43c09ee9b0e3",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b461a-4181-4995-8148-86e0d6c2c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn= KNeighborsClassifier()\n",
    "\n",
    "param_grid = {'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 13, 21, 34], 'weights': ['uniform', 'distance'], 'p': [1, 2, 3]}\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=LeaveOneOut())\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "accuracy_scores = cross_val_score(grid_search.best_estimator_, X, y, cv=LeaveOneOut())\n",
    "\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d210ad3-33b0-45bb-966d-5f2043957c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on testing data:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6be6f3-53c1-497b-affc-12b49151cb21",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e6af7-7c95-450e-b98b-2563ac4bd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear')  \n",
    "svm_model.fit(X, y)\n",
    "\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "accuracy_scores = cross_val_score(svm_model, X, y, scoring = accuracy, cv = 7)\n",
    "\n",
    "\n",
    "print(\"accuracy score:\", accuracy_scores)\n",
    "print(\"accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487625c-2c3a-4877-86db-291e1b81c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only best 80% of features\n",
    "features_coef = dict(zip(list(tfidf.get_feature_names()) + list(full_text[columns_of_quant_features].columns), list(svm_model.coef_[0]) + [0]*len(columns_of_quant_features))) # get feature names and coefficients, features must be from tfidf and also connected with quantitative variables\n",
    "top_features = sorted(features_coef.items(), key=lambda x: abs(x[1]), reverse=True)[:int(len(features_coef)*0.8)] # sort by absolute coefficient value and select top X %\n",
    "top_feature_names = [x[0] for x in top_features] # extract top feature names\n",
    "\n",
    "\n",
    "X_train_top_x = X[top_feature_names] # subset the data to include only top X % features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133cd5d-7c16-4887-8411-f61aa887a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "svm= SVC()\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 5, 10, 100], 'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], 'gamma': ['scale', 'auto', 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=LeaveOneOut(), verbose=1)\n",
    "\n",
    "grid_search.fit(X_train_top_x, y)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "accuracy_scores = cross_val_score(grid_search.best_estimator_, X_train_top_x, y, cv=LeaveOneOut())\n",
    "\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7b4a1-b205-4e8d-96cf-4cf65ae12f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test[top_feature_names])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on testing data:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96df1d9-b051-4438-8fcf-9d19e16232bf",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6bc74b-3ff9-4755-89cb-62e9cab979c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the decision tree classifier\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "accuracy_scores = cross_val_score(dtc, X, y, scoring=accuracy, cv=7)\n",
    "\n",
    "# Print the mean and standard deviation of the accuracy scores\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Accuracy score:\", accuracy_scores.mean())\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2e8e9-777c-4032-9756-91c72ea01a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "dtc.fit(X, y)\n",
    "importances = dtc.feature_importances_\n",
    "\n",
    "\n",
    "sorted_idx = np.argsort(importances)[::-1] # Get indices of the top features sorted in descending order of importance\n",
    "\n",
    "n_features = len(importances)\n",
    "n_keep = int(n_features * 0.2) # Compute the number of features to keep\n",
    "\n",
    "top_idx = sorted_idx[:n_keep] # Get the indices of the top n_keep features\n",
    "\n",
    "X_dtc_top_X = X.iloc[:, top_idx]\n",
    "\n",
    "print(f\"Original number of features: {n_features}\")\n",
    "print(f\"Number of features to keep: {n_keep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3f647-299e-40df-a7f0-60250d2b6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt= DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid = {'criterion': ['gini', 'entropy', 'log_loss'], 'max_depth': [None, 5, 10, 15, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4, 8], 'max_features': [None, 'sqrt', 'log2']}\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=LeaveOneOut(), verbose=1)\n",
    "\n",
    "grid_search.fit(X_dtc_top_X, y)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "accuracy_scores = cross_val_score(grid_search.best_estimator_, X_dtc_top_X, y, cv=LeaveOneOut())\n",
    "\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2193c752-d31b-409a-aedc-4e34a97d0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test.iloc[:, top_idx])\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on testing data:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efaccf1-0cde-4c58-8455-a2e955783199",
   "metadata": {},
   "source": [
    "## Naive Bayes, Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c881317-165f-4bbd-af80-efadade7377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "percentile = 50 # Select the top X % of features based on ANOVA F-value\n",
    "selector = SelectPercentile(f_classif, percentile=percentile)\n",
    "selector.fit(subset_tfidf_matrix, y)\n",
    "tfidf_selected = selector.transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe648c-7399-42dd-a73b-91858f550d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = selector.get_support(indices=True) # Get the indices of the selected features\n",
    "tfidf_df_selected = tfidf_df.iloc[:, selected_indices]\n",
    "tfidf_df_final = tfidf_df_selected.join(full_text[columns_of_quant_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fa9ae-8160-4aa8-90ae-697a4a98e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_final_test=tfidf_df_final.iloc[48:,]\n",
    "tfidf_df_final=tfidf_df_final.iloc[:48,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f248e5-a277-49f0-b0b0-af7476587991",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df_final[tfidf_df_final.columns.difference(['Solvency_ratio_bucket'])]\n",
    "y = tfidf_df_final['Solvency_ratio_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4d94b-fd1b-4bb2-bcb0-2fc14d008fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_df_final_test[tfidf_df_final_test.columns.difference(['Solvency_ratio_bucket'])]\n",
    "y_test = tfidf_df_final_test['Solvency_ratio_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af8e67-2059-428b-89ee-b318638a7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb=GaussianNB()  \n",
    "\n",
    "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "grid_search = GridSearchCV(nb, param_grid, cv=LeaveOneOut(), verbose=1)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "accuracy_scores = cross_val_score(grid_search.best_estimator_, X, y, cv=LeaveOneOut())\n",
    "\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_scores)\n",
    "print(\"Mean accuracy:\", np.mean(accuracy_scores))\n",
    "print(\"Accuracy std:\", np.std(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f660e-044d-421d-8130-3a210a3dad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on testing data:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6aa8f8-10f8-46b5-9422-8bbcf9b9857f",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3c37d-4050-413c-ab59-e0a48a5392db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the Keras model builder function\n",
    "def create_model(optimizer='adam', activation='relu', hidden1=128, hidden2=64):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(hidden1, activation=activation, input_shape=(X.shape[1],)),\n",
    "        keras.layers.Dense(hidden2, activation=activation),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the Keras classifier object\n",
    "keras_clf = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = param_grid = {'batch_size': [64, 128], 'epochs': [5, 10], 'optimizer': ['adam', 'sgd'], 'activation': ['relu', 'tanh'], 'hidden1': [64, 128], 'hidden2': [64]}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=keras_clf, param_grid=param_grid, cv=LeaveOneOut(), verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566f7fc-ef93-49a3-9cd7-9b9b6f8d3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on testing data:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "47db62abc0913d39599ca66e7022dc2dc64c0109f8e5bad1147b31e387c3ff29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
